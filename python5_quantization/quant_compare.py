import torch
import time
import os
from transformers import GPT2Tokenizer, GPT2LMHeadModel

device = "cpu"

# è·¯å¾„é…ç½®
orig_model_path = "../python3_distillation/gpt2_student_v2"
quant_model_path = "./gpt2_student_v2_quantized"
tokenizer = GPT2Tokenizer.from_pretrained(orig_model_path)

# åŠ è½½åŸå§‹æ¨¡å‹
model_orig = GPT2LMHeadModel.from_pretrained(orig_model_path).to(device).eval()

# åŠ è½½é‡åŒ–æ¨¡å‹
model_quant = GPT2LMHeadModel.from_pretrained(quant_model_path)
model_quant = torch.quantization.quantize_dynamic(model_quant, {torch.nn.Linear}, dtype=torch.qint8)
model_quant.load_state_dict(torch.load(os.path.join(quant_model_path, "pytorch_model.bin"), map_location=device))
model_quant.to(device).eval()

# å¯¹æ¯”å‡½æ•°
def benchmark(model, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    start = time.time()
    with torch.no_grad():
        out = model(**inputs)
    end = time.time()
    token_id = int(out.logits[0, -1].argmax())
    token_text = tokenizer.decode([token_id]).strip()
    return token_id, token_text, (end - start) * 1000

prompts = [
    "Hello world",
    "The sky is",
    "I love",
    "Artificial intelligence is",
    "Python is a popular"
]

# è¾“å‡ºå¯¹æ¯”ç»“æœ
for i, prompt in enumerate(prompts, 1):
    id1, text1, time1 = benchmark(model_orig, prompt)
    id2, text2, time2 = benchmark(model_quant, prompt)
    match = "âœ”ï¸ ä¸€è‡´" if id1 == id2 else "âŒ ä¸åŒ"
    speedup = time1 - time2
    print(f"\nğŸ“ Prompt {i}: {prompt}")
    print(f"  åŸå§‹æ¨¡å‹: {text1:<10} â± {time1:.2f} ms")
    print(f"  é‡åŒ–æ¨¡å‹: {text2:<10} â± {time2:.2f} ms")
    print(f"  âœ… è¾“å‡ºæ˜¯å¦ä¸€è‡´: {match} | åŠ é€Ÿ: {speedup:.2f} ms")
