# quant_infer.py

import torch
import time
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# ==== é…ç½® ====
model_path = "./gpt2_student_v2_quantized"
tokenizer_path = "../python3_distillation/gpt2_student_v2"
device = "cpu"  # INT8 åŠ¨æ€é‡åŒ–ä¸€èˆ¬åœ¨ CPU ä¸Šæ¨ç†æ•ˆæœæ›´ä½³, GPU ä¸æ”¯æŒ INT8 åŠ¨æ€é‡åŒ–

# ==== åŠ è½½ tokenizer ====
tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)

'''
Transformers çš„ from_pretrained() ä¼šåšä»€ä¹ˆï¼Ÿ
åŠ è½½ config.json æ„å»ºæ¨¡å‹ç»“æ„ï¼ˆé»˜è®¤ float32ï¼Œæœªé‡åŒ–ç»“æ„ï¼‰ï¼›
åŠ è½½ pytorch_model.bin æƒé‡ï¼ˆä½ ä¿å­˜çš„æ˜¯ INT8 å€¼ï¼‰ï¼›
ä¸ä¼šä¿®æ”¹ç»“æ„ï¼Œå³ä½¿æƒé‡æ˜¯ INT8ã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œæƒé‡è™½ç„¶å˜äº†ï¼Œä½†ç»“æ„è¿˜æ˜¯åŸå§‹çš„ GPT2 æ¨¡å‹ï¼Œæ²¡æœ‰ä»»ä½•é‡åŒ–å±‚çš„æ›¿æ¢ï¼

| åœºæ™¯                         | è¾“å…¥æ¨¡å‹çš„æƒé‡ | è¾“å…¥æ¨¡å‹çš„ç»“æ„ | `quantize_dynamic` åšäº†ä»€ä¹ˆ               |
| -------------------------- | ------- | ------- | ------------------------------------- |
| âœ… ç¬¬ä¸€æ¬¡ï¼ˆ`quantize_model.py`ï¼‰ | float32 | åŸå§‹ç»“æ„    | âœ… é‡åŒ–æƒé‡ + âœ… æ›¿æ¢ç»“æ„                       |
| âœ… ç¬¬äºŒæ¬¡ï¼ˆ`quant_infer.py`ï¼‰    | INT8 æƒé‡ | åŸå§‹ç»“æ„    | âŒ ä¸å†é‡åŒ–ï¼ˆæƒé‡å·²æ˜¯ INT8ï¼‰<br>âœ… åªæ˜¯æ›¿æ¢ç»“æ„ï¼ˆè®©ç»“æ„å¯¹å¾—ä¸Šï¼‰ |

torch.save(model.state_dict()) ä¿å­˜çš„æ˜¯æƒé‡å‚æ•°ï¼ˆå­—å…¸å½¢å¼ï¼‰ï¼Œä¸åŒ…å«ç»“æ„ä¿¡æ¯ã€‚
æ‰€ä»¥æ¨ç†æ—¶ä½ éœ€è¦ï¼š
ç”¨ GPT2LMHeadModel.from_pretrained(...) å…ˆæ¢å¤åŸå§‹ç»“æ„ï¼ˆéé‡åŒ–çš„ Linearï¼‰ï¼›
å†ç”¨ quantize_dynamic(...) æ›¿æ¢ç»“æ„ï¼›
ç„¶å load_state_dict(...) æ¢å¤ INT8 æƒé‡ã€‚
'''
# ==== åˆå§‹åŒ–æ¨¡å‹ç»“æ„ + åŠ è½½æƒé‡ ====
print("ğŸš€ åŠ è½½ INT8 é‡åŒ–æ¨¡å‹ä¸­...")
model = GPT2LMHeadModel.from_pretrained(model_path)
model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
model.load_state_dict(torch.load(f"{model_path}/pytorch_model.bin", map_location=device))  # åŠ è½½æƒé‡
model.to(device).eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ==== æ¨ç†å‡½æ•° ====
def infer(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        start = time.time()
        outputs = model(**inputs)
        end = time.time()
    logits = outputs.logits
    token_id = int(logits[0, -1].argmax())
    token_text = tokenizer.decode([token_id], clean_up_tokenization_spaces=True).strip()
    elapsed = (end - start) * 1000
    return token_id, token_text, elapsed

# ==== å¤šç»„æµ‹è¯• ====
prompts = [
    "Hello world",
    "The sky is",
    "I love",
    "Artificial intelligence is",
    "Python is a popular"
]

for i, prompt in enumerate(prompts, 1):
    token_id, token_text, elapsed = infer(prompt)
    print(f"\nğŸ“ Prompt {i}: {prompt}")
    print(f"ğŸ”¹ Predicted Token ID: {token_id}")
    print(f"ğŸ”¹ Predicted Token Text: {token_text}")
    print(f"â±ï¸ æ¨ç†è€—æ—¶: {elapsed:.2f} ms")
