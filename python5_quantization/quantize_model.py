# quantize_model.py

import os
import torch
from transformers import GPT2LMHeadModel

# ==== è·¯å¾„é…ç½® ====
model_path = "../python3_distillation/gpt2_student_v2"
save_path = "./gpt2_student_v2_quantized"
os.makedirs(save_path, exist_ok=True)

# ==== åŠ è½½åŸå§‹å°æ¨¡å‹ ====
print("ğŸš€ åŠ è½½å°æ¨¡å‹ student_v2...")
model = GPT2LMHeadModel.from_pretrained(model_path)
model.eval()

# ==== åŠ¨æ€é‡åŒ–ï¼ˆLinear å±‚ï¼‰ ====
print("âš™ï¸ å¼€å§‹åŠ¨æ€é‡åŒ–æ¨¡å‹...")
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # ä»…é‡åŒ– Linear å±‚
    dtype=torch.qint8   # è½¬ä¸º INT8 æƒé‡
)

print("âœ… é‡åŒ–å®Œæˆ")

# ==== ä¿å­˜é‡åŒ–æ¨¡å‹ï¼ˆPyTorch åŸç”Ÿæ–¹å¼ï¼‰ ====
print(f"ğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹åˆ°: {save_path}")

# torch.save(model.state_dict()) ä¿å­˜çš„æ˜¯æƒé‡å‚æ•°ï¼ˆå­—å…¸å½¢å¼ï¼‰ï¼Œä¸åŒ…å«ç»“æ„ä¿¡æ¯
torch.save(quantized_model.state_dict(), os.path.join(save_path, "pytorch_model.bin"))

# æ‹·è´åŸå§‹ config.jsonï¼ˆå¦åˆ™ transformers æ— æ³•åŠ è½½ï¼‰
import shutil
shutil.copy(os.path.join(model_path, "config.json"), os.path.join(save_path, "config.json"))

print("ğŸ é‡åŒ–æµç¨‹ç»“æŸï¼")

