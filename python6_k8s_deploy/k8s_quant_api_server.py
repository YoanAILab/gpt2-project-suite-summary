from flask import Flask, request, jsonify
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
import os

app = Flask(__name__)

# âœ… æ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨é‡åŒ–ç‰ˆæ¨¡å‹ç›®å½•ï¼‰
MODEL_PATH = "./gpt2_student_v2_quantized"

print("ğŸš€ åŠ è½½é‡åŒ– student_v2 æ¨¡å‹ä¸­...")
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)
model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)
model.eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

@app.route("/", methods=["GET"])
def index():
    return "Welcome to the GPT-2 Quantized API Server"

@app.route("/infer", methods=["POST"])
def infer():
    try:
        data = request.get_json()
        prompt = data.get("prompt", "")
        print(f"ğŸ“¥ æ”¶åˆ° prompt: {prompt}")

        if not prompt:
            return jsonify({"error": "Missing prompt"}), 400

        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        pred_id = int(torch.argmax(logits[0, -1]))
        pred_token = tokenizer.decode([pred_id], clean_up_tokenization_spaces=True).strip()

        return jsonify({"response": f"æ¨¡å‹ç”Ÿæˆè¯: {pred_token}", "token_id": pred_id})

    except Exception as e:
        print("âŒ æ¨ç†å¤±è´¥:", e)
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # âœ… å¯åŠ¨æœåŠ¡ç›‘å¬æ‰€æœ‰åœ°å€ï¼ˆç”¨äºå®¹å™¨æš´éœ²ï¼‰
    app.run(host="0.0.0.0", port=6006)
