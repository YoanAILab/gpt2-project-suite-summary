# copied from: python1_basic_training/transformers3_DeploymentFlask.py
# modified for: docker deployment
# date: 2025-04-27

from flask import Flask, request, jsonify, render_template
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ğŸš€ Using device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU only âŒ")

# åˆå§‹åŒ–æ¨¡å‹
# model.train() ç”¨äºè®­ç»ƒé˜¶æ®µï¼Œä¼šæ›´æ–°æƒé‡ï¼Œæ¢¯åº¦è®¡ç®—åæ›´æ–°å‚æ•°ï¼Œæ¨¡å‹é»˜è®¤å€¼
# model.eval() ç”¨äºéªŒè¯/æ¨ç†ï¼Œä¸ä¼šæ›´æ–°æƒé‡
model_path = "gpt2_finetune"
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token   # âœ… æå‰è®¾ç½® pad_tokenï¼Œé¿å… warning
model = GPT2LMHeadModel.from_pretrained(model_path).to(device).eval()
# model = GPT2LMHeadModel.from_pretrained(model_path)
# model = model.to(device)
# model.eval()

# æ¨ç†å‡½æ•°
def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)
    inputs['attention_mask'] = inputs['attention_mask']  # âœ… æ˜¾å¼ä¼ å…¥ attention_mask
    with torch.no_grad():   # ä¸´æ—¶ä½œç”¨åŸŸï¼Œç”¨äºä¸´æ—¶å…³é—­æ¢¯åº¦è®¡ç®—ï¼ˆæé«˜é€Ÿåº¦ï¼ŒèŠ‚çœå†…å­˜ï¼‰
        outputs = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_length=80,
            do_sample=True,
            top_k=40,
            top_p=0.9,
            temperature=0.7,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# æ„é€  Flask åº”ç”¨ï¼Œåˆå§‹åŒ– Flask Web æœåŠ¡å™¨
app = Flask(__name__)

# åŠ è½½ templates/index.html
@app.route("/")
def index():
    return render_template("index.html")

# API è·¯ç”±
@app.route("/generate", methods=["POST"])
def generate():
    data = request.get_json()
    prompt = data.get("prompt", "")
    if not prompt:
        return jsonify({"error": "Prompt is required"}), 400

    output = generate_response(prompt)
    return jsonify({"response": output})

@app.route("/status", methods=["GET"])
def status():
    return jsonify({
        "gpu": torch.cuda.is_available(),
        "device": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
    })

# å¯åŠ¨ Flask æœåŠ¡ï¼ˆç›‘å¬å…¨éƒ¨ IPï¼‰
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
