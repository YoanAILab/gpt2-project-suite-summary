import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
import time
from transformers import GPT2Tokenizer

# TensorRT æ˜¯ä¸“é—¨ä¸º ONNX æˆ– TensorRT Engine æ ¼å¼ è®¾è®¡çš„, åŸæ¥åœ¨ gpt2_finetune ä¸­çš„æ¨¡å‹æ— æ³•å¤„ç†
# åˆå§‹åŒ– Logger å’Œè·¯å¾„ TRT_LOGGERï¼šTensorRT æ—¥å¿—å¯¹è±¡ï¼Œåªè¾“å‡º ERROR çº§åˆ«åŠä»¥ä¸Šçš„æ—¥å¿—
TRT_LOGGER = trt.Logger(trt.Logger.ERROR)
onnx_path = "model/gpt2.onnx"
model_path = "../python1_basic_training/gpt2_finetune"

'''
ç¨‹åºå¯åŠ¨
 â†“
åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆmake_contextï¼‰
 â†“
æ¨ç†æ—¶ push è¿›å…¥
 â†“
æ‰§è¡Œ TensorRT æ¨ç†
 â†“
æ¨ç†å®Œ pop å‡ºæ¥

æ˜¾å¼ç®¡ç† CUDA ä¸Šä¸‹æ–‡æ˜¯å› ä¸ºï¼š
TensorRT æ¨ç†ä¸­ï¼Œæ˜¾å¡èµ„æºåˆ†é…è¦ç²¾å‡†æ§åˆ¶ï¼Œå¦åˆ™å¤šçº¿ç¨‹æˆ–ä¸åŒç¨‹åºå®¹æ˜“å‡ºé”™ã€‚
'''
# === åˆå§‹åŒ–æ˜¾å¼ CUDA context ===
cuda.init()  # åˆå§‹åŒ–CUDA
DEVICE = cuda.Device(0)  # é€‰ç¬¬0å·æ˜¾å¡
CTX = DEVICE.make_context()  # åˆ›å»ºä¸€ä¸ªæ˜¾å¼ä¸Šä¸‹æ–‡
# å¦‚æœä¸æ‰‹åŠ¨åˆ›å»ºï¼ŒPyTorchã€TensorFlowè¿™ç±»æ¡†æ¶å†…éƒ¨ä¼šå¸®ä½ éšå¼åˆ›å»ºï¼Œä¸å¥½æ§åˆ¶

'''
å®šä¹‰æ„å»º TensorRT å¼•æ“çš„å‡½æ•°
åˆ›å»º builderã€networkã€parserï¼Œä¸‰ä»¶å¥—ã€‚
EXPLICIT_BATCHï¼šæ˜ç¡®æŒ‡å®š batch ç»´åº¦ï¼ˆå¿…è¦çš„ï¼Œå¦åˆ™ ONNX å’Œ TensorRTç†è§£çš„ç»´åº¦å¯¹ä¸ä¸Šï¼‰ã€‚
åŒæ—¶æ‰“å¼€ä¸‰æ ·ä¸œè¥¿ï¼š
ä¸€ä¸ª Builderï¼ˆå»ºé€ å™¨ï¼‰ â” builder
ä¸€ä¸ª Networkï¼ˆç¥ç»ç½‘ç»œç»“æ„ï¼‰ â” network
ä¸€ä¸ª Parserï¼ˆè§£æå™¨ï¼‰ â” parser
ç”¨æ¥åç»­æ„å»º TensorRT å¼•æ“ã€‚

trt.Builder(TRT_LOGGER)	æ–°å»ºä¸€ä¸ª TensorRT çš„"å»ºé€ å™¨"ï¼Œå¯ä»¥å»ºå¼•æ“ã€å»ºé…ç½®
builder.create_network(flags)	åˆ›å»ºä¸€ä¸ªæ–°çš„ç¥ç»ç½‘ç»œï¼ˆNetworkï¼‰ï¼Œflagså‚æ•°æŒ‡å®šæ˜¯å¦ä½¿ç”¨æ˜¾å¼ batchï¼ˆå¾ˆé‡è¦ï¼‰
trt.OnnxParser(network, TRT_LOGGER)	åŸºäºåˆšæ‰çš„ networkï¼Œæ–°å»ºä¸€ä¸ª ONNX è§£æå™¨ï¼Œå¯ä»¥æŠŠ ONNX æ–‡ä»¶è§£æåˆ° Network é‡Œé¢
âœ… å»º Builder â” âœ… å»º Network â” âœ… ç”¨ Parser æŠŠ ONNX å¯¼å…¥ Network
1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) æ„æ€æ˜¯æŠŠ1å¾€å·¦ç§»åŠ¨ EXPLICIT_BATCH ä½
1 << 1 â†’ 00000010
1 << 2 â†’ 00000100
1 << 3 â†’ 00001000

0000 0000   # æ‰€æœ‰åŠŸèƒ½éƒ½å…³
0000 0010   # å¼€äº† EXPLICIT_BATCHï¼ˆç¬¬1ä½ï¼‰
0000 0100   # å¼€äº† FP16ï¼ˆç¬¬2ä½ï¼‰
0000 1000   # å¼€äº† INT8ï¼ˆç¬¬3ä½ï¼‰
0000 0010 | 0000 0100 = 0000 0110  ï¼ˆ= 6ï¼‰åŒæ—¶å¼€EXPLICIT_BATCHå’ŒFP16

"with" æ˜¯Pythonçš„è¯­æ³•ç³–ï¼Œç”¨æ¥
ğŸ‘‰ è‡ªåŠ¨ç®¡ç†èµ„æºçš„ç”³è¯·å’Œé‡Šæ”¾ï¼
TensorRTå¯¹è±¡ï¼ˆbuilderã€networkã€parserï¼‰éƒ½æ˜¯C++åº•å±‚å¯¹è±¡
å¦‚æœä½ è‡ªå·±å¿˜äº† .destroy()ï¼Œå°±ä¼šé€ æˆæ˜¾å­˜æ³„éœ²ã€ç¨‹åºå´©æºƒ
ç”¨ withï¼ŒPythonä¼šè‡ªåŠ¨å¸®ä½ é”€æ¯èµ„æºï¼Œéå¸¸å®‰å…¨ï¼

with å¼€å¯
  â†“
Builder (è´Ÿè´£å»ºå¼•æ“)
  â†“
Network (æ­ç½‘ç»œ)
  â†“
Parser (è¯»ONNXæ–‡ä»¶åˆ°Networké‡Œ)
  â†“
with é€€å‡ºæ—¶è‡ªåŠ¨é‡Šæ”¾èµ„æº
'''
# === æ„å»º TensorRT å¼•æ“ ===
def build_engine(onnx_file_path):
    with trt.Builder(TRT_LOGGER) as builder, \
         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \
         trt.OnnxParser(network, TRT_LOGGER) as parser:

        # åˆ›å»ºä¸€ä¸ª TensorRT æ„å»ºé…ç½®å¯¹è±¡ï¼ˆconfigï¼‰ config é‡Œé¢å¯ä»¥é…ç½®å¾ˆå¤šé€‰é¡¹
        # ä½è¿ç®—ï¼Œè¡¨ç¤º 2Â³â° = 1073741824 å­—èŠ‚ï¼Œå³ 1GBï¼Œè®¾ç½® TensorRT çš„ä¸­é—´è®¡ç®—ä¸´æ—¶ç©ºé—´ï¼ˆworkspaceï¼‰æœ€å¤§åªèƒ½ç”¨ 1GB å†…å­˜
        config = builder.create_builder_config()
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)

        # é…ç½®åŠ¨æ€ Shape Profile è¾“å…¥å°ºå¯¸çš„èŒƒå›´å®šä¹‰
        '''
        profile.set_shape("input_ids", (1, 8), (1, 16), (1, 32))
        æœ€å°è¾“å…¥	(1, 8)	batch=1, sequence_length=8ï¼ˆæœ€çŸ­è¾“å…¥8ä¸ªtokenï¼‰
        æœ€ä¼˜è¾“å…¥	(1, 16)	batch=1, sequence_length=16ï¼ˆå¸¸è§æƒ…å†µï¼‰
        æœ€å¤§è¾“å…¥	(1, 32)	batch=1, sequence_length=32ï¼ˆæœ€é•¿æ”¯æŒ32ä¸ªtokenï¼‰
        '''
        profile = builder.create_optimization_profile()
        profile.set_shape("input_ids", (1, 8), (1, 16), (1, 32))
        profile.set_shape("attention_mask", (1, 8), (1, 16), (1, 32))
        config.add_optimization_profile(profile)

        # åŠ è½½å¹¶è§£æ ONNX
        with open(onnx_file_path, "rb") as model:
            if not parser.parse(model.read()):
                print("âŒ ONNX è§£æå¤±è´¥")
                for error in range(parser.num_errors):
                    print(parser.get_error(error))
                return None

        # è¿”å›ç¼–è¯‘å¥½çš„ Engine çœŸæ­£å¼€å§‹ç¼–è¯‘æˆ TensorRT äºŒè¿›åˆ¶æ‰§è¡Œè®¡åˆ’
        return builder.build_engine(network, config)


# æ¨¡å—çº§çš„å…¨å±€å˜é‡ï¼Œåªè¦ trt_infer è¿™ä¸ªæ¨¡å—è¢« importï¼Œè¿™äº›å¯¹è±¡å°±ä¼šåœ¨ç¬¬ä¸€æ¬¡åŠ è½½æ—¶åˆå§‹åŒ–å¥½å¹¶å¸¸é©»å†…å­˜ã€‚
print("ğŸš€ åˆå§‹åŒ– TensorRT å¼•æ“å’Œåˆ†è¯å™¨...")
ENGINE = build_engine(onnx_path)  # TensorRTæ‰§è¡Œå¼•æ“, è½»åˆ™å‡ åç§’ï¼Œé‡åˆ™å‡ åˆ†é’Ÿ, æˆ‘çš„è·‘äº†70s
# å¦‚æœä¿å­˜æˆ .plan æ–‡ä»¶ï¼ˆè¿™æ˜¯ TensorRTå¼•æ“çš„åç¼€ï¼‰ï¼Œåé¢æ¯æ¬¡éƒ¨ç½²æ—¶ï¼Œç›´æ¥ååºåˆ—åŒ–åŠ è½½ .plan æ–‡ä»¶ï¼Œå‡ ä¹æ˜¯ç§’çº§å¯åŠ¨çš„
# åˆ«çš„æ–‡ä»¶å¯¼å…¥ è¯¥æ–‡ä»¶ trt_infer çš„æ—¶å€™ï¼ŒPython ä¼šè‡ªåŠ¨æ‰§è¡Œ ENGINE = build_engine(onnx_path)ï¼
CONTEXT = ENGINE.create_execution_context()  # æ‰§è¡Œæ—¶ç”¨çš„ context
TOKENIZER = GPT2Tokenizer.from_pretrained(model_path)  # æ–‡æœ¬åˆ†è¯å™¨

# trt_infer.py ä½œä¸ºä¸€ä¸ªæ¨¡å—æ¥ä½¿ç”¨çš„ï¼Œå¦‚æœç›´æ¥è¿è¡Œ python trt_infer.py	åˆå§‹åŒ–å®Œå°±é€€å‡ºäº†ï¼ˆå› ä¸ºæ²¡æœ‰è°ƒç”¨æ¨ç†ï¼‰
# import trt_infer	æŠŠ ENGINEã€CONTEXTã€infer_tensorrt åŠ è½½åˆ°å†…å­˜ï¼Œè€Œä¸ä¼šç›´æ¥æ‰§è¡Œä»»ä½•å‡½æ•°è°ƒç”¨ï¼Œç­‰ä½ éœ€è¦çš„æ—¶å€™æ‰è°ƒç”¨
# è°ƒç”¨ infer_tensorrt(prompt)	æ‰‹åŠ¨æ‰§è¡Œæ¨ç†ï¼Œæ˜¾å¼ push / pop ä¸Šä¸‹æ–‡ï¼Œæ¨ç†ç»“æŸåèµ„æºé‡Šæ”¾
# === æ¨ç†å‡½æ•° ===
def infer_tensorrt(prompt):
    CTX.push()  # âœ… æ˜¾å¼è¿›å…¥ä¸Šä¸‹æ–‡
    # æ‰‹åŠ¨ "æ¿€æ´»" CUDA ä¸Šä¸‹æ–‡ï¼Œä¿è¯æ‰€æœ‰æ˜¾å­˜ç”³è¯·å’Œå†…å­˜æ‹·è´éƒ½åœ¨è‡ªå·±å¼€çš„ context ä¸‹æ‰§è¡Œ

    # æŠŠ prompt ç¼–æˆ input_ids å’Œ attention_maskï¼Œè¿”å› numpy æ ¼å¼ã€‚
    # å¹¶è½¬ä¸º int32ï¼ˆTensorRT è¦æ±‚çš„ç²¾åº¦ï¼‰ã€‚
    # numpyæ ¼å¼ï¼ŒæŒ‡çš„æ˜¯ ç”¨numpyè¿™ä¸ªåº“è¡¨ç¤ºçš„æ•°æ®æ ¼å¼ å¯ä»¥é«˜æ•ˆåœ°åœ¨å†…å­˜é‡Œè¿ç»­å­˜å‚¨ï¼Œéå¸¸é€‚åˆæ‹¿æ¥åšæ•°å€¼è®¡ç®—ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ æ¨ç†ç­‰ã€‚
    try:
        inputs = TOKENIZER(prompt, return_tensors="np", padding="max_length", truncation=True, max_length=16)
        input_ids = inputs["input_ids"].astype(np.int32)
        attention_mask = inputs["attention_mask"].astype(np.int32)

        # æ ¹æ®æ¨¡å‹é‡Œç»‘å®šçš„åå­—ï¼ŒæŸ¥åˆ°å¯¹åº”ç´¢å¼•ï¼Œæ¨ç†æ—¶è¦ç”¨åˆ°ã€‚
        input_ids_bind = ENGINE.get_binding_index("input_ids")
        attention_bind = ENGINE.get_binding_index("attention_mask")
        output_bind = ENGINE.get_binding_index("logits")

        # æ˜ç¡®å‘Šè¯‰ TensorRTï¼šè¿™æ¬¡æ¨ç†ç”¨çš„ input shape æ˜¯å¤šå°‘ï¼ˆåŠ¨æ€ batch æ”¯æŒï¼‰ã€‚
        CONTEXT.set_binding_shape(input_ids_bind, input_ids.shape)
        CONTEXT.set_binding_shape(attention_bind, attention_mask.shape)

        # ç”³è¯·æ˜¾å­˜ï¼Œè¿˜åœ¨ CPU ä¸Šå»ºäº†ä¸ªç©ºçš„ output æ•°ç»„ï¼Œç”¨æ¥æ‹¿å›æ¨ç†ç»“æœ
        d_input_ids = cuda.mem_alloc(input_ids.nbytes)
        d_attention = cuda.mem_alloc(attention_mask.nbytes)
        output_shape = (1, 16, ENGINE.get_binding_shape(output_bind)[-1])
        output = np.empty(output_shape, dtype=np.float32)
        d_output = cuda.mem_alloc(output.nbytes)

        cuda.memcpy_htod(d_input_ids, input_ids)
        cuda.memcpy_htod(d_attention, attention_mask)

        # æ‰§è¡Œæ¨ç†ï¼Œè®¡æ—¶
        start = time.time()
        CONTEXT.execute_v2([int(d_input_ids), int(d_attention), int(d_output)])
        end = time.time()

        # æ‹·è´è¾“å‡ºå›ä¸»æœºå†…å­˜
        cuda.memcpy_dtoh(output, d_output)
        elapsed = (end - start) * 1000

        return output, elapsed

    finally:
        CTX.pop()  # âœ… æ¨ç†å®Œæˆåé‡Šæ”¾ä¸Šä¸‹æ–‡ï¼Œæ— è®ºæˆåŠŸå¤±è´¥ï¼Œæœ€åéƒ½è¦ pop()ï¼Œé‡Šæ”¾æ‰ CUDA ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢èµ„æºæ³„éœ²ï¼
