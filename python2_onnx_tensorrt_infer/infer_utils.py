import time
import numpy as np
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

'''
åš PyTorch å’Œ TensorRT æ¨ç†æ•ˆæœå¯¹æ¯”çš„æµ‹è¯•è„šæœ¬ã€‚

è¿™ä¸ª infer_utils.py å®Œå…¨æ²¡æœ‰ç”¨åˆ°ä¹‹å‰çš„ trt_infer.pyã€‚
å®ƒæ˜¯è‡ªå·±é‡æ–°å†™äº†ä¸€å¥—æ¨ç†æµç¨‹ï¼Œè€Œä¸”æ˜¯ç‹¬ç«‹è¿è¡Œçš„ã€‚
ğŸ”µ æ›´å…·ä½“æ¥è¯´ï¼š
infer_utils.py é‡Œé¢è‡ªå·±é‡æ–°å®ç°äº†ï¼š
TensorRTå¼•æ“æ„å»ºï¼ˆbuild_trt_engineï¼‰
TensorRTæ¨ç†ï¼ˆinfer_tensorrtï¼‰
è€Œä¸æ˜¯å» import trt_infer ç›´æ¥ç”¨ä½ ä¹‹å‰å®šä¹‰å¥½çš„ infer_tensorrt å‡½æ•°ã€‚
å®ƒè¿ from trt_infer import infer_tensorrt è¿™ç§å¯¼å…¥éƒ½æ²¡æœ‰ã€‚
âœ… æ‰€ä»¥æ•´ä¸ª infer_utils.py å…¶å®æ˜¯è‡ªç»™è‡ªè¶³çš„ï¼Œè·Ÿ trt_infer.py å®Œå…¨æ²¡å…³ç³»ï¼Œä¹Ÿä¸ä¼šè°ƒç”¨å®ƒé‡Œé¢çš„å‡½æ•°ã€‚
'''

# åˆ›å»ºä¸€ä¸ª TensorRT çš„æ—¥å¿—å¯¹è±¡
TRT_LOGGER = trt.Logger(trt.Logger.INFO)

# ====== é€šç”¨é…ç½® ======
model_path = "../python1_basic_training/gpt2_finetune"
onnx_path = "model/gpt2.onnx"
prompt = "Hello world"

# è¿”å› logits + æ¨ç†ç”¨æ—¶ï¼ˆæ¯«ç§’msï¼‰
# ====== PyTorch æ¨ç† ======
def infer_pytorch(prompt):
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2LMHeadModel.from_pretrained(model_path).eval()
    inputs = tokenizer(prompt, return_tensors="pt")

    with torch.no_grad():
        start = time.time()
        outputs = model(**inputs)
        end = time.time()

    logits = outputs.logits.detach().numpy()
    elapsed_ms = (end - start) * 1000
    return logits, elapsed_ms


# ====== TensorRT æ¨ç† ======
def build_trt_engine(onnx_path):
    with trt.Builder(TRT_LOGGER) as builder, \
            builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \
            trt.OnnxParser(network, TRT_LOGGER) as parser:
        config = builder.create_builder_config()
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)

        profile = builder.create_optimization_profile()
        profile.set_shape("input_ids", (1, 8), (1, 10), (1, 16))
        profile.set_shape("attention_mask", (1, 8), (1, 10), (1, 16))
        config.add_optimization_profile(profile)

        with open(onnx_path, "rb") as model_file:
            parser.parse(model_file.read())

        return builder.build_engine(network, config)


def infer_tensorrt(prompt):
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    inputs = tokenizer(prompt, return_tensors="np", padding="max_length", max_length=10)
    input_ids = inputs["input_ids"].astype(np.int32)
    attention_mask = inputs["attention_mask"].astype(np.int32)

    engine = build_trt_engine(onnx_path)
    context = engine.create_execution_context()

    input_ids_bind = engine.get_binding_index("input_ids")
    attention_bind = engine.get_binding_index("attention_mask")
    output_bind = engine.get_binding_index("logits")

    context.set_binding_shape(input_ids_bind, input_ids.shape)
    context.set_binding_shape(attention_bind, attention_mask.shape)

    d_input_ids = cuda.mem_alloc(input_ids.nbytes)
    d_attention = cuda.mem_alloc(attention_mask.nbytes)
    output_shape = (1, 10, engine.get_binding_shape(output_bind)[-1])
    output = np.empty(output_shape, dtype=np.float32)
    d_output = cuda.mem_alloc(output.nbytes)

    cuda.memcpy_htod(d_input_ids, input_ids)
    cuda.memcpy_htod(d_attention, attention_mask)

    start = time.time()
    context.execute_v2([int(d_input_ids), int(d_attention), int(d_output)])
    end = time.time()

    cuda.memcpy_dtoh(output, d_output)

    elapsed_ms = (end - start) * 1000
    return output, elapsed_ms

'''
åœ¨åˆ†ç±»ã€ç”Ÿæˆç±»æ¨¡å‹æ¨ç†æ—¶ï¼Œæ¯”å¦‚ GPT-2 è¾“å‡º logitsï¼ˆå°±æ˜¯æ¯ä¸ªtokençš„å¾—åˆ†ï¼‰ï¼Œ
æˆ‘ä»¬é€šå¸¸å…³å¿ƒçš„æ˜¯ï¼šå¾—åˆ†æœ€é«˜çš„é‚£ä¸ª token æ˜¯å“ªä¸ªï¼Ÿ
è¿™ä¸ªå¾—åˆ†æœ€é«˜çš„ï¼Œå°±æ˜¯æ‰€è°“çš„ï¼š
ğŸ”µ Top-1ï¼ˆTop-1 Predictionï¼‰
'''
# ====== å¯¹æ¯”å‡½æ•° ======
def compare_infer(prompt):
    print(f"\nğŸš€ å¯¹æ¯”æ¨ç†ç»“æœï¼ˆprompt: '{prompt}'ï¼‰")

    logits_pt, time_pt = infer_pytorch(prompt)
    logits_trt, time_trt = infer_tensorrt(prompt)

    print(f"ğŸ”¹ PyTorch è€—æ—¶ï¼š{time_pt:.2f} ms")
    print(f"ğŸ”¹ TensorRT è€—æ—¶ï¼š{time_trt:.2f} ms")

    top1_pt = logits_pt[0, -1].argmax()
    top1_trt = logits_trt[0, -1].argmax()
    print(f"ğŸ”¹ Top-1 Token (PyTorch)   : {top1_pt}")
    print(f"ğŸ”¹ Top-1 Token (TensorRT) : {top1_trt}")
    print(f"âœ… è¾“å‡ºæ˜¯å¦ä¸€è‡´ï¼š{'âœ… æ˜¯' if top1_pt == top1_trt else 'âŒ å¦'}")

'''
é‚£ä¹ˆå¦‚æœä½ ç›´æ¥å†™ compare_infer(prompt)ï¼Œ
import ä¹Ÿä¼šç›´æ¥å¼€å§‹è·‘æ¨ç†ï¼Œé‚£å°±ä¼šå¾ˆå°´å°¬ï¼ˆäººå®¶åªæ˜¯æƒ³å¼•ç”¨ä¸ªå°å·¥å…·ï¼Œä¸æ˜¯è¦è·‘ä½ çš„æµ‹è¯•ï¼‰ã€‚

åœ¨æ¯ä¸€ä¸ª Python æ–‡ä»¶ä¸­ï¼ŒPython é»˜è®¤ä¼šç»™ä½ æä¾›ä¸€ä¸ªå†…ç½®å˜é‡ __name__
ç›´æ¥è¿è¡Œè¿™ä¸ªæ–‡ä»¶ï¼ˆpython xxx.pyï¼‰  å†…ç½®å˜é‡ __name__ ==	"__main__"
è¿™ä¸ªæ–‡ä»¶è¢«åˆ«äºº import äº†ï¼ˆæ¯”å¦‚ import xxxï¼‰	"æ¨¡å—å"ï¼Œä¹Ÿå°±æ˜¯æ–‡ä»¶åï¼ˆä¸å¸¦ .pyï¼‰ å†…ç½®å˜é‡ __name__ == "æ¨¡å—å"
'''
# ====== è¿è¡Œæµ‹è¯• ======
if __name__ == "__main__":
    compare_infer(prompt)
