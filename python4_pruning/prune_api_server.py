# prune_api_server.py

from flask import Flask, request, jsonify, render_template
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
import os

# ==== æ¨¡å‹ä¸tokenizerè·¯å¾„ ====
model_path = "./gpt2_student_v2_pruned"
tokenizer_path = "../python3_distillation/gpt2_student_v2"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ==== åˆå§‹åŒ– Flask åº”ç”¨ ====
app = Flask(__name__, static_folder="static", template_folder="templates")

# ==== åŠ è½½æ¨¡å‹ ====
print("ğŸš€ åŠ è½½å‰ªæå student_v2 æ¨¡å‹ä¸­...")
model = GPT2LMHeadModel.from_pretrained(model_path).to(device).eval()
tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ==== ç½‘é¡µå…¥å£ ====
@app.route("/", methods=["GET"])
def home():
    return render_template("index.html")

# ==== æ¨ç†æ¥å£ ====
@app.route("/infer", methods=["POST"])
def infer():
    try:
        data = request.get_json()
        prompt = data.get("prompt", "").strip()

        if not prompt:
            return jsonify({"error": "Missing prompt"}), 400

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        token_id = int(logits[0, -1].argmax())
        token_text = tokenizer.decode([token_id], clean_up_tokenization_spaces=True).strip()

        if not token_text:
            token_text = f"[token_id={token_id}]"

        return jsonify({
            "response": f"å‰ªææ¨¡å‹ç”Ÿæˆè¯: {token_text}",
            "token_id": token_id
        })

    except Exception as e:
        print("âŒ æ¨ç†å¤±è´¥:", e)
        return jsonify({"error": str(e)}), 500

# ==== å¯åŠ¨æœåŠ¡ ====
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 6006))
    app.run(host="0.0.0.0", port=port)
