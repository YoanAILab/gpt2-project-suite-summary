# prune_compare.py

import torch
import time
from transformers import GPT2Tokenizer, GPT2LMHeadModel

device = "cuda" if torch.cuda.is_available() else "cpu"

# ==== æ¨¡å‹è·¯å¾„ ====
model_orig_path = "../python3_distillation/gpt2_student_v2"
model_prune_path = "./gpt2_student_v2_pruned"
tokenizer_path = model_orig_path  # tokenizer æ— éœ€å‰ªæ

# ==== åŠ è½½ Tokenizer ====
tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)

# ==== åŠ è½½æ¨¡å‹ ====
print("ğŸš€ åŠ è½½åŸå§‹ student_v2 æ¨¡å‹...")
model_orig = GPT2LMHeadModel.from_pretrained(model_orig_path).to(device).eval()

print("ğŸš€ åŠ è½½å‰ªæåæ¨¡å‹...")
model_prune = GPT2LMHeadModel.from_pretrained(model_prune_path).to(device).eval()

# ==== æµ‹è¯• Prompt åˆ—è¡¨ ====
prompts = [
    "Hello world",
    "The sky is",
    "I love",
    "Artificial intelligence is",
    "Python is a popular"
]

# ==== æ¨ç†å‡½æ•° ====
def benchmark(model, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    start = time.time()
    with torch.no_grad():
        outputs = model(**inputs)
    end = time.time()
    logits = outputs.logits
    token_id = int(logits[0, -1].argmax())
    token_text = tokenizer.decode([token_id], clean_up_tokenization_spaces=True).strip()
    return token_id, token_text, (end - start) * 1000  # è¿”å› ms

# ==== å¯¹æ¯”æµ‹è¯• ====
# enumerate æ˜¯ Python å†…ç½®å‡½æ•°ï¼Œç”¨æ¥åœ¨ éå†å¯è¿­ä»£å¯¹è±¡ï¼ˆå¦‚åˆ—è¡¨ã€å…ƒç»„ã€å­—ç¬¦ä¸²ï¼‰æ—¶ï¼ŒåŒæ—¶è·å–ç´¢å¼•å’Œå…ƒç´ æœ¬èº«ï¼Œå¸¸ç”¨äº for å¾ªç¯ä¸­ã€‚
print("\nğŸ§ª å¼€å§‹å‰ªæå‰åæ¨ç†å¯¹æ¯”...\n")
for i, prompt in enumerate(prompts, 1):
    id1, text1, time1 = benchmark(model_orig, prompt)
    id2, text2, time2 = benchmark(model_prune, prompt)

    consistency = "âœ”ï¸ ä¸€è‡´" if id1 == id2 else "âŒ ä¸ä¸€è‡´"
    speedup = (time1 - time2) / time1 * 100

    print(f"ğŸ“ Prompt {i}: {prompt}")
    print(f"  ğŸŸ© åŸå§‹æ¨¡å‹: [{text1}] (ID={id1}) â± {time1:.2f} ms")
    print(f"  ğŸŸ¥ å‰ªææ¨¡å‹: [{text2}] (ID={id2}) â± {time2:.2f} ms")
    print(f"  ğŸ” ç»“æœä¸€è‡´æ€§: {consistency}")
    print(f"  âš¡ï¸ æ¨ç†åŠ é€Ÿ: {speedup:.1f}%\n")
