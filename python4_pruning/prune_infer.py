# prune_infer.py

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# ==== é…ç½® ====
model_path = "./gpt2_student_v2_pruned"
tokenizer_path = "../python3_distillation/gpt2_student_v2"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ==== åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ====
print("ğŸš€ åŠ è½½å‰ªæå student_v2 æ¨¡å‹ä¸­...")
model = GPT2LMHeadModel.from_pretrained(model_path).to(device).eval()
tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ==== æ¨ç†å‡½æ•° ====
def infer(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    last_token_id = int(logits[0, -1].argmax())
    last_token = tokenizer.decode([last_token_id], clean_up_tokenization_spaces=True).strip()
    return last_token_id, last_token

# ==== å¤šç»„æµ‹è¯• ====
prompts = [
    "Hello world",
    "The sky is",
    "I love",
    "Artificial intelligence is"
]

for i, prompt in enumerate(prompts, 1):
    token_id, token_text = infer(prompt)
    print(f"\nğŸ“ Prompt {i}: {prompt}")
    print(f"ğŸ”¹ Predicted Token ID: {token_id}")
    print(f"ğŸ”¹ Predicted Token Text: {token_text}")
