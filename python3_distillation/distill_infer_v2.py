# distill_infer_v2.py

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# 1. åŠ è½½å°æ¨¡å‹ student
MODEL_DIR = "./gpt2_student_v2"  # å°æ¨¡å‹ç›®å½•
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"ğŸš€ åŠ è½½å° Student æ¨¡å‹ä¸­...")
model = GPT2LMHeadModel.from_pretrained(MODEL_DIR).to(device).eval()
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)
print(f"âœ… å° Student æ¨¡å‹åŠ è½½å®Œæˆ")

# 2. æ¨ç†å‡½æ•°ï¼ˆæ”¯æŒå•æ¡ or å¤šæ¡ promptï¼‰
def infer(prompts, max_length=30):
    if isinstance(prompts, str):
        prompts = [prompts]  # å•æ¡è½¬åˆ—è¡¨

    results = []

    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = model(**inputs)

        logits = outputs.logits
        last_logits = logits[:, -1, :]
        top1_token_id = torch.argmax(last_logits, dim=-1).item()
        top1_token = tokenizer.decode([top1_token_id], clean_up_tokenization_spaces=True).strip()

        results.append({
            "prompt": prompt,
            "top1_token_id": top1_token_id,
            "top1_token": top1_token
        })

    return results

# 3. æµ‹è¯•å…¥å£
if __name__ == "__main__":
    test_prompts = [
        "Hello world",
        "The sky is",
        "I love",
        "Artificial intelligence is"
    ]

    outputs = infer(test_prompts)

    for i, output in enumerate(outputs):
        print(f"\nğŸ“ Prompt {i+1}: {output['prompt']}")
        print(f"ğŸ”¹ Predicted Token ID: {output['top1_token_id']}")
        print(f"ğŸ”¹ Predicted Token Text: {output['top1_token']}")
