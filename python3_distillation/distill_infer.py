# distill_infer.py

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# 1. åŠ è½½ student æ¨¡å‹
MODEL_DIR = "./gpt2_student"  # è®­ç»ƒåä¿å­˜çš„studentæ¨¡å‹è·¯å¾„
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"ğŸš€ åŠ è½½ student æ¨¡å‹ä¸­...")
model = GPT2LMHeadModel.from_pretrained(MODEL_DIR).to(device).eval()
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)
print(f"âœ… student æ¨¡å‹åŠ è½½å®Œæˆ")


# 2. æ¨ç†å‡½æ•°ï¼ˆæ”¯æŒå•æ¡ or å¤šæ¡promptï¼‰
def infer(prompts, max_length=30):
    if isinstance(prompts, str):
        prompts = [prompts]  # è½¬æˆåˆ—è¡¨ï¼Œç»Ÿä¸€å¤„ç†

    results = []

    # å¯¹æ¯ä¸ª prompt è¿›è¡Œåˆ†è¯ï¼Œå¹¶è½¬ä¸º PyTorch æ ¼å¼ tensor, ç§»åŠ¨åˆ° GPU/CPU ä¸Šã€‚
    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = model(**inputs)

        # å¾—åˆ°æ¯ä¸ª token çš„é¢„æµ‹ logits åˆ†æ•°ã€‚
        logits = outputs.logits  # [batch_size, sequence_length, vocab_size]

        # å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹, argmax æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„ token ID
        last_logits = logits[:, -1, :]  # åªå–æœ€åä¸€å±‚
        top1_token_id = torch.argmax(last_logits, dim=-1).item()

        # å°† token ID è§£ç æˆäººç±»å¯è¯»çš„ token æ–‡æœ¬
        top1_token = tokenizer.decode([top1_token_id], clean_up_tokenization_spaces=True).strip()

        # å°†å½“å‰ prompt çš„æ¨ç†ç»“æœä¿å­˜ä¸‹æ¥ï¼ˆåŒ…å«åŸå§‹ promptã€token idã€token æ–‡å­—ï¼‰ã€‚
        results.append({
            "prompt": prompt,
            "top1_token_id": top1_token_id,
            "top1_token": top1_token
        })

    return results


# 3. æµ‹è¯•å…¥å£
if __name__ == "__main__":  # è¡¨æ˜ä¸‹é¢çš„ä»£ç åªåœ¨ç›´æ¥è¿è¡Œè„šæœ¬æ—¶ç”Ÿæ•ˆï¼ˆä¸ä¼šåœ¨è¢« import æ—¶æ‰§è¡Œï¼‰ã€‚
    test_prompts = [
        "Hello world",
        "The sky is",
        "I love",
        "Artificial intelligence is"
    ]

    outputs = infer(test_prompts)

    for i, output in enumerate(outputs):
        print(f"\nğŸ“ Prompt {i + 1}: {output['prompt']}")  # è¾“å…¥ promptï¼›
        print(f"ğŸ”¹ Predicted Token ID: {output['top1_token_id']}") # æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€ä¸ª token çš„ IDï¼›
        print(f"ğŸ”¹ Predicted Token Text: {output['top1_token']}")  # è¯¥ token å¯¹åº”çš„æ–‡æœ¬ã€‚
