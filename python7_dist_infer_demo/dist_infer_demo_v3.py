import os
import torch
import torch.distributed as dist
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import time

def setup_distributed():
    dist.init_process_group(
        backend="gloo",  # CPU/å•GPU æ¨è glooï¼ŒNCCL ç”¨äºå¤š GPU
        init_method="env://"
    )
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    return rank, world_size

def cleanup():
    dist.destroy_process_group()

def main():
    rank, world_size = setup_distributed()
    torch.manual_seed(42)

    # è®¾å¤‡åˆ†é…ç­–ç•¥
    if torch.cuda.is_available() and rank == 0:
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    # æ¨¡å‹åŠ è½½
    model_path = "./gpt2_student_v2"
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2LMHeadModel.from_pretrained(model_path).to(device).eval()

    prompts = [
        "Hello world",
        "The sky is",
        "I love",
        "Artificial intelligence is"
    ]

    if rank >= len(prompts):
        print(f"[Rank {rank}] ğŸš« æ— ä»»åŠ¡ï¼Œç›´æ¥é€€å‡º")
        cleanup()
        return

    prompt = prompts[rank]
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    with torch.no_grad():
        start = time.time()
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=20,
            do_sample=False, # é»˜è®¤æ¨èéƒ¨ç½²ç¯å¢ƒè®¾ç½®ä¸º Falseï¼ˆç¨³å®šè¾“å‡ºã€å¯å¤ç°ï¼‰ï¼Œè€Œåˆ›æ„ç”Ÿæˆåˆ™è®¾ä¸º True
            use_cache=True  # âœ… å¼€å¯ KV Cacheï¼Œåœ¨â€œé€ token å¤šè½®ç”Ÿæˆâ€æˆ–â€œé•¿æ–‡æœ¬+å¤šè½®äº¤äº’â€çš„åœºæ™¯ä¸­ï¼ŒKV Cache ä¼šæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚
        )
        end = time.time()

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    latency_ms = (end - start) * 1000

    result = {
        "rank": rank,
        "prompt": prompt,
        "generated_text": generated_text,
        "latency_ms": latency_ms
    }

    gathered = [None for _ in range(world_size)]
    dist.all_gather_object(gathered, result)

    if rank == 0:
        print("\nğŸš€ æ‰€æœ‰è¿›ç¨‹æ¨ç†ç»“æœï¼ˆä½¿ç”¨ KV Cacheï¼‰:")
        for r in sorted(gathered, key=lambda x: x["rank"]):
            print(f"\n[Rank {r['rank']}] ğŸ“ Prompt: {r['prompt']}")
            print(f"[Rank {r['rank']}] ğŸ”¹ Generated: {r['generated_text']}")
            print(f"[Rank {r['rank']}] â± æ¨ç†è€—æ—¶: {r['latency_ms']:.2f} ms")

    cleanup()

if __name__ == "__main__":
    main()
