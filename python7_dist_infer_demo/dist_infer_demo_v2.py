import os
import torch
import torch.distributed as dist
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import time

'''
RANK=0            # ç¬¬ä¸€ä¸ªè¿›ç¨‹æ˜¯ rank 0ï¼Œç¬¬äºŒä¸ªæ˜¯ 1ï¼Œä»¥æ­¤ç±»æ¨
WORLD_SIZE=4      # å¯åŠ¨äº† 4 ä¸ªè¿›ç¨‹
MASTER_ADDR=127.0.0.1
MASTER_PORT=29500
'''
def setup_distributed():
    dist.init_process_group(
        backend="gloo",  # Windows å’Œ CPU ç¯å¢ƒæ¨è glooï¼ŒLinux GPU å¤šå¡å»ºè®®ç”¨ nccl
        init_method="env://"  # å‘Šè¯‰ PyTorch ä½¿ç”¨ç¯å¢ƒå˜é‡æ–¹å¼å»ºç«‹é€šä¿¡è¿æ¥ï¼Œè¿è¡Œ torchrun ç³»ç»Ÿè‡ªåŠ¨ä¼šè®¾ç½®ä¸Šè¾¹çš„ç¯å¢ƒå˜é‡
    )
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    return rank, world_size

# æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
def cleanup():
    dist.destroy_process_group()

def main():
    rank, world_size = setup_distributed()
    torch.manual_seed(42) # è®¾ç½®éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼ˆseedï¼‰ï¼Œä»¥ç¡®ä¿ç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´ã€å¯å¤ç°

    # === è®¾å¤‡åˆ†é…ç­–ç•¥ ===
    if torch.cuda.is_available() and rank == 0:
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    # === åŠ è½½æ¨¡å‹ä¸ tokenizer ===
    model_path = "./gpt2_student_v2"
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2LMHeadModel.from_pretrained(model_path).to(device).eval()

    # æ€»å…±æœ‰ 4 ä¸ª promptï¼Œåˆ†åˆ«åˆ†é…ç»™ 4 ä¸ªè¿›ç¨‹
    prompts = [
        "Hello world",
        "The sky is",
        "I love",
        "Artificial intelligence is"
    ]

    # å¦‚æœæŸä¸ªè¿›ç¨‹çš„ rank è¶…å‡ºäº† prompt æ•°é‡ï¼Œç›´æ¥é€€å‡º
    if rank >= len(prompts):
        print(f"[Rank {rank}] ğŸš« æ— ä»»åŠ¡ï¼Œç›´æ¥é€€å‡º")
        cleanup()
        return

    prompt = prompts[rank]
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        start = time.time()
        outputs = model(**inputs)
        end = time.time()

    logits = outputs.logits
    pred_token_id = int(logits[0, -1].argmax())
    pred_token = tokenizer.decode([pred_token_id]).strip()
    latency_ms = (end - start) * 1000

    result = {
        "rank": rank,
        "prompt": prompt,
        "pred_token_id": pred_token_id,
        "pred_token": pred_token,
        "latency_ms": latency_ms
    }

    # === è¿›ç¨‹é—´é€šä¿¡åŒæ­¥ç»“æœ ===
    # ä½¿ç”¨ all_gather_object å°†æ¯ä¸ªè¿›ç¨‹çš„ result åŒæ­¥åˆ°æ‰€æœ‰è¿›ç¨‹ã€‚
    # gathered æœ€ç»ˆä¸ºä¸€ä¸ªåŒ…å«æ‰€æœ‰è¿›ç¨‹æ¨ç†ä¿¡æ¯çš„åˆ—è¡¨ã€‚
    gathered = [None for _ in range(world_size)]
    dist.all_gather_object(gathered, result)

    if rank == 0:
        print("\nğŸš€ æ‰€æœ‰è¿›ç¨‹æ¨ç†ç»“æœï¼š")
        # å°†æ‰€æœ‰è¿›ç¨‹æ”¶é›†åˆ°çš„æ¨ç†ç»“æœï¼ˆgatheredï¼‰ï¼ŒæŒ‰ rank é¡ºåºæ’åˆ—ï¼Œå¹¶ä¾æ¬¡æ‰“å°æ¯ä¸ªè¿›ç¨‹çš„ç»“æœ
        for r in sorted(gathered, key=lambda x: x["rank"]):
            print(f"\n[Rank {r['rank']}] ğŸ“ Prompt: {r['prompt']}")
            print(f"[Rank {r['rank']}] ğŸ”¹ Predicted Token ID: {r['pred_token_id']}")
            print(f"[Rank {r['rank']}] ğŸ”¹ Predicted Token Text: {r['pred_token']}")
            print(f"[Rank {r['rank']}] â± æ¨ç†è€—æ—¶: {r['latency_ms']:.2f} ms")

    cleanup()

if __name__ == "__main__":
    main()
