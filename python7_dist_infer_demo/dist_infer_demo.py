# dist_infer_demo.py
import os
import time
import torch
import torch.distributed as dist  # PyTorch åˆ†å¸ƒå¼è®­ç»ƒ/æ¨ç†æ¨¡å—
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼Œä½¿ç”¨ gloo é€šä¿¡åç«¯ï¼ˆCPUä¸Šæœ€å¸¸ç”¨ï¼‰
def setup_distributed():
    dist.init_process_group(backend="gloo")
    rank = dist.get_rank()  # å½“å‰è¿›ç¨‹ç¼–å·
    world_size = dist.get_world_size()  # æ€»è¿›ç¨‹æ•°
    return rank, world_size

# æ¸…ç†åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼Œé‡Šæ”¾èµ„æº
def cleanup_distributed():
    dist.destroy_process_group()

def load_model_and_tokenizer(model_path):
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2LMHeadModel.from_pretrained(model_path)
    model.eval()
    return tokenizer, model

# æ¨ç†å‡½æ•°ï¼ˆæœªåœ¨ main ä¸­ä½¿ç”¨ï¼‰
def infer(prompt, model, tokenizer):
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        start = time.time()
        outputs = model(**inputs)
        end = time.time()
    logits = outputs.logits
    pred_token_id = int(torch.argmax(logits[0, -1]))
    pred_token = tokenizer.decode([pred_token_id])
    elapsed_ms = (end - start) * 1000
    return pred_token_id, pred_token.strip(), elapsed_ms

def main():
    rank, world_size = setup_distributed()

    prompts = [
        "Hello world",
        "The sky is",
        "I love",
        "Artificial intelligence is",
        "Python is a popular"
    ]
    prompt = prompts[rank % len(prompts)]

    # rank 0 è¿›ç¨‹ä½œä¸ºâ€œä¸»è¿›ç¨‹â€ï¼Œæ‰“å°æ€»ä½“ä¿¡æ¯
    if rank == 0:
        print(f"ğŸš€ ä½¿ç”¨ GPT2 Student v2 è¿›è¡Œå¤šè¿›ç¨‹æ¨ç†ï¼ˆå…± {world_size} ä¸ªè¿›ç¨‹ï¼‰")

    # ä»… rank 0 ç”¨ GPUï¼Œå…¶ä½™è¿›ç¨‹ä½¿ç”¨ CPU
    device = torch.device("cuda" if torch.cuda.is_available() and rank == 0 else "cpu")
    model_path = "./gpt2_student_v2"
    tokenizer, model = load_model_and_tokenizer(model_path)
    model.to(device)

    # å°†è¾“å…¥ prompt è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    '''
    å‡å¦‚
    prompt = "Hello world"
    inputs = tokenizer(prompt, return_tensors="pt")
    ä¸Šé¢ä¸€å¥æ‰§è¡Œåï¼Œinputs ä¾¿æ˜¯
    {
        'input_ids': tensor([[15496, 995]], device='cuda:0'),
        'attention_mask': tensor([[1, 1]], device='cuda:0')
    }
    '''
    inputs = tokenizer(prompt, return_tensors="pt")
    '''
    ä¸‹é¢çš„å­—å…¸æ¨å¯¼å¼ç­‰ä»·äº
    new_inputs = {}
    for k, v in inputs.items():
        new_inputs[k] = v.to(device)
    inputs = new_inputs
    æ–°çš„ inputs åªæ˜¯æŠŠåŸæ¥é”®å€¼å¯¹ä¸­å€¼çš„ device ç­‰äº cpu æˆ– gpu äº†
    '''
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        start = time.time()
        outputs = model(**inputs)
        end = time.time()

    logits = outputs.logits
    pred_id = int(torch.argmax(logits[0, -1]))
    pred_token = tokenizer.decode([pred_id])
    elapsed = (end - start) * 1000

    time.sleep(rank * 0.1)  # åŠ å…¥è½»å¾®å»¶è¿Ÿï¼Œé”™å¼€å¤šè¿›ç¨‹çš„æ‰“å°è¾“å‡ºé¡ºåºï¼Œé˜²æ­¢å®ƒä»¬åœ¨æ§åˆ¶å°æŠ¢è¾“å‡ºè€Œå¯¼è‡´æ··ä¹±
    print("\n" + "=" * 60)
    print(f"[Rank {rank}] ğŸ“ Prompt: {prompt}")
    print(f"[Rank {rank}] ğŸ”¹ Predicted Token ID: {pred_id}")
    print(f"[Rank {rank}] ğŸ”¹ Predicted Token Text: {pred_token.strip()}")
    print(f"[Rank {rank}] â± æ¨ç†è€—æ—¶: {elapsed:.2f} ms")
    print("=" * 60 + "\n")

    cleanup_distributed()

if __name__ == "__main__":
    main()
